# ğŸ«€ Cardiolog: Explainable AI for Medical Decision Support
### A Neuro-Symbolic System combining Machine Learning and Logical Reasoning

![Python](https://img.shields.io/badge/Language-Python_3.x-3776AB?logo=python&logoColor=white)
![Logic](https://img.shields.io/badge/Reasoning-s(CASP)-blue)
![ML](https://img.shields.io/badge/AI-Scikit_Learn-F7931E?logo=scikit-learn)
![Focus](https://img.shields.io/badge/Focus-XAI_%2F_Healthcare-green)

> **Cardiolog** is a medical decision-support system designed to address the **lack of interpretability and accountability in clinical AI**.  
> Instead of returning opaque risk scores, the system provides **explicit, human-readable justifications** for each prediction using a **neuro-symbolic architecture**.

The project explores how statistical learning can be translated into **logical, inspectable reasoning pipelines**, making AI outputs more suitable for high-stakes and regulated environments.

---

## ğŸš€ Key Capabilities

* **ğŸ” Glass-Box Decision Support**  
  Translates opaque Random Forest decision paths into transparent logical rules that can be inspected, audited, and debated.

* **ğŸ—£ï¸ Explainable Justifications**  
  Generates structured, natural-language explanations such as:  
  *â€œHigh risk BECAUSE cholesterol > 240 AND age > 50â€*, enabling clinician-level understanding of model behavior.

* **ğŸ§  Hybrid Intelligence Architecture**  
  Combines the predictive strengths of **Machine Learning** with the reasoning and traceability of **symbolic logic (Prolog / s(CASP))**.

* **ğŸ§¬ Dynamic Patient Modeling**  
  New patient data is automatically converted into logical facts, enabling real-time inference and explanation without retraining the model.

---

## ğŸ—ï¸ System Architecture

The system is organized as a three-layer pipeline that converts statistical patterns into logical proofs:

### 1. Learning Layer â€” Statistical Pattern Extraction
* Trains a **Random Forest Classifier** on the Cleveland Heart Disease dataset.
* Captures predictive structure while accepting limited interpretability at this stage.

### 2. Translation Layer â€” Model-to-Logic Compilation
* A custom **Tree-to-Logic compiler** (`translator.py`) traverses each decision tree and transpiles its paths into **s(CASP)-compatible predicates**.
* Each tree is treated as an independent reasoning agent, contributing partial evidence.

### 3. Reasoning Layer â€” Symbolic Inference
* Aggregates logical rules and patient-specific facts.
* Executes queries using **s(CASP)** to compute stable models.
* Produces a structured **justification tree**, rendered as HTML for inspection.

---

## ğŸ› ï¸ Project Structure

```text
/
â”œâ”€â”€ cardiolog/           # Core source code
â”‚   â”œâ”€â”€ translator.py    # Decision Tree â†’ Logic compiler
â”‚   â”œâ”€â”€ main.py          # Pipeline orchestration and inference
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                # Clinical datasets (CSV)
â”œâ”€â”€ prolog/              # Generated logical knowledge base
â”œâ”€â”€ templates/           # HTML explanation templates
â””â”€â”€ output/              # Reasoning traces and justification trees
